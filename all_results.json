{
  "runs": [
    {
      "run_id": 1,
      "hypothesis": "Files with empty content (no text) might contain bad data.",
      "filter_code": "len(content.strip()) == 0",
      "filter_result": {
        "filtered_count": 11600,
        "sample_evaluations": [
          [
            "doc_14167.txt",
            false
          ],
          [
            "doc_07351.txt",
            false
          ],
          [
            "doc_01726.txt",
            false
          ],
          [
            "doc_29752.txt",
            false
          ],
          [
            "doc_35177.txt",
            true
          ]
        ],
        "first_matches": [
          "doc_35177.txt",
          "doc_14258.txt",
          "doc_20795.txt",
          "doc_26239.txt",
          "doc_12840.txt"
        ],
        "output_path": "./test_results/junk_data_v2/junk_data_run1.txt",
        "summary": "Saved 11600 matching filenames to './test_results/junk_data_v2/junk_data_run1.txt'."
      },
      "confidence_result": {
        "confidence": 1.0,
        "bad": 5,
        "total": 5,
        "summary": "Confidence: 1.00, Bad: 5, Total: 5",
        "metrics": {
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "accuracy": 1.0,
          "confidence": 1.0,
          "verdict": {
            "text": "Excellent - This hypothesis very effectively identifies bad data",
            "score": 1.0
          }
        }
      },
      "metrics": {
        "precision": 1.0,
        "recall": 1.0,
        "f1_score": 1.0,
        "accuracy": 1.0,
        "confidence": 1.0,
        "verdict": {
          "text": "Excellent - This hypothesis very effectively identifies bad data",
          "score": 1.0
        }
      },
      "timestamp": "2025-09-03 22:52:58",
      "is_unique": true
    },
    {
      "run_id": 2,
      "hypothesis": "Files with filenames that contain a sequence of numbers followed by \".txt\" but have no content may contain bad data.",
      "filter_code": "any(c.isdigit() for c in fname)",
      "filter_result": {
        "filtered_count": 36718,
        "sample_evaluations": [
          [
            "doc_14167.txt",
            true
          ],
          [
            "doc_07351.txt",
            true
          ],
          [
            "doc_01726.txt",
            true
          ],
          [
            "doc_29752.txt",
            true
          ],
          [
            "doc_35177.txt",
            true
          ]
        ],
        "first_matches": [
          "doc_14167.txt",
          "doc_07351.txt",
          "doc_01726.txt",
          "doc_29752.txt",
          "doc_35177.txt"
        ],
        "output_path": "./test_results/junk_data_v2/junk_data_run2.txt",
        "summary": "Saved 36718 matching filenames to './test_results/junk_data_v2/junk_data_run2.txt'."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 5,
        "summary": "Confidence: 0.00, Bad: 0, Total: 5",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.88,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.88,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 22:53:03",
      "is_unique": true
    },
    {
      "run_id": 3,
      "hypothesis": "Files with filenames that have a length greater than 10 characters and no content may contain bad data.",
      "filter_code": "len(fname) > 10 and (not content or len(content.strip()) == 0)",
      "filter_result": {
        "filtered_count": 11600,
        "sample_evaluations": [
          [
            "doc_14167.txt",
            false
          ],
          [
            "doc_07351.txt",
            false
          ],
          [
            "doc_01726.txt",
            false
          ],
          [
            "doc_29752.txt",
            false
          ],
          [
            "doc_35177.txt",
            true
          ]
        ],
        "first_matches": [
          "doc_35177.txt",
          "doc_14258.txt",
          "doc_20795.txt",
          "doc_26239.txt",
          "doc_12840.txt"
        ],
        "output_path": "./test_results/junk_data_v2/junk_data_run3.txt",
        "summary": "Saved 11600 matching filenames to './test_results/junk_data_v2/junk_data_run3.txt'."
      },
      "confidence_result": {
        "confidence": 1.0,
        "bad": 5,
        "total": 5,
        "summary": "Confidence: 1.00, Bad: 5, Total: 5",
        "metrics": {
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "accuracy": 1.0,
          "confidence": 1.0,
          "verdict": {
            "text": "Excellent - This hypothesis very effectively identifies bad data",
            "score": 1.0
          }
        }
      },
      "metrics": {
        "precision": 1.0,
        "recall": 1.0,
        "f1_score": 1.0,
        "accuracy": 1.0,
        "confidence": 1.0,
        "verdict": {
          "text": "Excellent - This hypothesis very effectively identifies bad data",
          "score": 1.0
        }
      },
      "timestamp": "2025-09-03 22:53:07",
      "is_unique": true
    },
    {
      "run_id": 4,
      "hypothesis": "Files that contain a significantly higher number of numerical values within the content may contain bad data.",
      "filter_code": "any(c.isdigit() for c in fname)",
      "filter_result": {
        "filtered_count": 36718,
        "sample_evaluations": [
          [
            "doc_14167.txt",
            true
          ],
          [
            "doc_07351.txt",
            true
          ],
          [
            "doc_01726.txt",
            true
          ],
          [
            "doc_29752.txt",
            true
          ],
          [
            "doc_35177.txt",
            true
          ]
        ],
        "first_matches": [
          "doc_14167.txt",
          "doc_07351.txt",
          "doc_01726.txt",
          "doc_29752.txt",
          "doc_35177.txt"
        ],
        "output_path": "./test_results/junk_data_v2/junk_data_run4.txt",
        "summary": "Saved 36718 matching filenames to './test_results/junk_data_v2/junk_data_run4.txt'."
      },
      "confidence_result": {
        "confidence": 1.0,
        "bad": 4,
        "total": 4,
        "summary": "Confidence: 1.00, Bad: 4, Total: 4",
        "metrics": {
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "accuracy": 1.0,
          "confidence": 1.0,
          "verdict": {
            "text": "Excellent - This hypothesis very effectively identifies bad data",
            "score": 1.0
          }
        }
      },
      "metrics": {
        "precision": 1.0,
        "recall": 1.0,
        "f1_score": 1.0,
        "accuracy": 1.0,
        "confidence": 1.0,
        "verdict": {
          "text": "Excellent - This hypothesis very effectively identifies bad data",
          "score": 1.0
        }
      },
      "timestamp": "2025-09-03 22:53:12",
      "is_unique": true
    },
    {
      "run_id": 5,
      "hypothesis": "Files that contain an unusually high frequency of special characters within their content may contain bad data.",
      "filter_code": "len([c for c in content if not c.isalnum() and not c.isspace()]) > len(content) * 0.2 if len(content) > 5 else False",
      "filter_result": {
        "filtered_count": 2898,
        "sample_evaluations": [
          [
            "doc_14167.txt",
            false
          ],
          [
            "doc_07351.txt",
            false
          ],
          [
            "doc_01726.txt",
            false
          ],
          [
            "doc_29752.txt",
            false
          ],
          [
            "doc_35177.txt",
            false
          ]
        ],
        "first_matches": [
          "doc_18561.txt",
          "doc_19330.txt",
          "doc_24740.txt",
          "doc_25994.txt",
          "doc_04593.txt"
        ],
        "output_path": "./test_results/junk_data_v2/junk_data_run5.txt",
        "summary": "Saved 2898 matching filenames to './test_results/junk_data_v2/junk_data_run5.txt'."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 5,
        "summary": "Confidence: 0.00, Bad: 0, Total: 5",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.88,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.88,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 22:53:18",
      "is_unique": true
    }
  ],
  "best_run": {
    "run_id": 1,
    "hypothesis": "Files with empty content (no text) might contain bad data.",
    "filter_code": "len(content.strip()) == 0",
    "filter_result": {
      "filtered_count": 11600,
      "sample_evaluations": [
        [
          "doc_14167.txt",
          false
        ],
        [
          "doc_07351.txt",
          false
        ],
        [
          "doc_01726.txt",
          false
        ],
        [
          "doc_29752.txt",
          false
        ],
        [
          "doc_35177.txt",
          true
        ]
      ],
      "first_matches": [
        "doc_35177.txt",
        "doc_14258.txt",
        "doc_20795.txt",
        "doc_26239.txt",
        "doc_12840.txt"
      ],
      "output_path": "./test_results/junk_data_v2/junk_data_run1.txt",
      "summary": "Saved 11600 matching filenames to './test_results/junk_data_v2/junk_data_run1.txt'."
    },
    "confidence_result": {
      "confidence": 1.0,
      "bad": 5,
      "total": 5,
      "summary": "Confidence: 1.00, Bad: 5, Total: 5",
      "metrics": {
        "precision": 1.0,
        "recall": 1.0,
        "f1_score": 1.0,
        "accuracy": 1.0,
        "confidence": 1.0,
        "verdict": {
          "text": "Excellent - This hypothesis very effectively identifies bad data",
          "score": 1.0
        }
      }
    },
    "metrics": {
      "precision": 1.0,
      "recall": 1.0,
      "f1_score": 1.0,
      "accuracy": 1.0,
      "confidence": 1.0,
      "verdict": {
        "text": "Excellent - This hypothesis very effectively identifies bad data",
        "score": 1.0
      }
    },
    "timestamp": "2025-09-03 22:52:58",
    "is_unique": true
  },
  "overall_verdict": {
    "overall_score": 1.0,
    "best_hypotheses": [
      {
        "run_id": 1,
        "hypothesis": "Files with empty content (no text) might contain bad data.",
        "f1_score": 1.0,
        "precision": 1.0,
        "recall": 1.0,
        "verdict_score": 1.0,
        "verdict_text": "Excellent - This hypothesis very effectively identifies bad data"
      },
      {
        "run_id": 3,
        "hypothesis": "Files with filenames that have a length greater than 10 characters and no content may contain bad data.",
        "f1_score": 1.0,
        "precision": 1.0,
        "recall": 1.0,
        "verdict_score": 1.0,
        "verdict_text": "Excellent - This hypothesis very effectively identifies bad data"
      },
      {
        "run_id": 4,
        "hypothesis": "Files that contain a significantly higher number of numerical values within the content may contain bad data.",
        "f1_score": 1.0,
        "precision": 1.0,
        "recall": 1.0,
        "verdict_score": 1.0,
        "verdict_text": "Excellent - This hypothesis very effectively identifies bad data"
      }
    ],
    "recommendation": "Multiple strong hypotheses found. Recommend using the top hypothesis for filtering bad data."
  }
}