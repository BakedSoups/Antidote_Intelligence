{
  "runs": [
    {
      "run_id": 1,
      "hypothesis": "Files that have a significantly higher frequency of special characters compared to alphanumeric characters might contain bad data.",
      "filter_code": "any(not c.isalnum() and not c.isspace() for c in fname)",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run1.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:34:59",
      "is_unique": true
    },
    {
      "run_id": 2,
      "hypothesis": "Files that have a significantly higher number of repeated words or phrases compared to unique words might contain bad data.",
      "filter_code": "any(c.isdigit() for c in fname)",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run2.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:01",
      "is_unique": true
    },
    {
      "run_id": 3,
      "hypothesis": "Files that contain an unusually high number of non-standard or non-ASCII characters are likely to contain bad data.",
      "filter_code": "any(ord(c) > 127 for c in fname if c.isprintable())",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run3.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:02",
      "is_unique": true
    },
    {
      "run_id": 4,
      "hypothesis": "Files that have an unusually high frequency of one-letter words compared to multi-letter words might contain bad data.",
      "filter_code": "len([word for word in fname.split() if len(word) == 1]) > len([word for word in fname.split() if len(word) > 1]) if len(fname.split()) > 0 else False",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run4.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:03",
      "is_unique": true
    },
    {
      "run_id": 5,
      "hypothesis": "Files that have a significantly higher proportion of sentences ending with exclamation marks compared to sentences ending with periods might contain bad data.",
      "filter_code": "(sum(1 for c in fname if c == '.') > 0) and (sum(1 for c in fname if c == '!') / (sum(1 for c in fname if c == '.') + 1) > 1)",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run5.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:05",
      "is_unique": true
    },
    {
      "run_id": 6,
      "hypothesis": "Files that have a significantly higher ratio of uppercase letters to lowercase letters in their content might contain bad data.",
      "filter_code": "sum(1 for c in fname if c.isalpha() and c.isupper()) > sum(1 for c in fname if c.isalpha() and c.islower()) * 2 if sum(1 for c in fname if c.isalpha() and c.islower()) > 0 else False",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run6.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:07",
      "is_unique": true
    },
    {
      "run_id": 7,
      "hypothesis": "Files that have an unusually high frequency of repeated special characters in their content might contain bad data.",
      "filter_code": "any(not c.isalnum() and not c.isspace() for c in fname)",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run7.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:09",
      "is_unique": true
    },
    {
      "run_id": 8,
      "hypothesis": "Files that have an unusually high frequency of repeated words or phrases in their content might contain bad data.",
      "filter_code": "any(fname.count(c) > 2 for c in set(fname) if c.isalpha())",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run8.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:10",
      "is_unique": true
    },
    {
      "run_id": 9,
      "hypothesis": "Files that contain a significantly higher number of non-alphanumeric characters compared to alphanumeric characters might contain bad data.",
      "filter_code": "sum(1 for c in fname if not c.isalnum()) > sum(1 for c in fname if c.isalnum()) * 2",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run9.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:11",
      "is_unique": true
    },
    {
      "run_id": 10,
      "hypothesis": "Files that have a significantly higher proportion of repeated consecutive vowels in their content might contain bad data.",
      "filter_code": "'aaa' in fname or 'eee' in fname or 'iii' in fname or 'ooo' in fname or 'uuu' in fname",
      "filter_result": {
        "filtered_count": 0,
        "sample_evaluations": [],
        "first_matches": [],
        "output_path": "./junk_data/junk_data_run10.txt",
        "summary": "No files matched the filter."
      },
      "confidence_result": {
        "confidence": 0.0,
        "bad": 0,
        "total": 0,
        "summary": "No files to analyze",
        "metrics": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0,
          "accuracy": 0.0,
          "confidence": 0.0,
          "verdict": {
            "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
            "score": 0.0
          }
        }
      },
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      },
      "timestamp": "2025-09-03 21:35:12",
      "is_unique": true
    }
  ],
  "best_run": {
    "run_id": 1,
    "hypothesis": "Files that have a significantly higher frequency of special characters compared to alphanumeric characters might contain bad data.",
    "filter_code": "any(not c.isalnum() and not c.isspace() for c in fname)",
    "filter_result": {
      "filtered_count": 0,
      "sample_evaluations": [],
      "first_matches": [],
      "output_path": "./junk_data/junk_data_run1.txt",
      "summary": "No files matched the filter."
    },
    "confidence_result": {
      "confidence": 0.0,
      "bad": 0,
      "total": 0,
      "summary": "No files to analyze",
      "metrics": {
        "precision": 0.0,
        "recall": 0.0,
        "f1_score": 0,
        "accuracy": 0.0,
        "confidence": 0.0,
        "verdict": {
          "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
          "score": 0.0
        }
      }
    },
    "metrics": {
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0,
      "accuracy": 0.0,
      "confidence": 0.0,
      "verdict": {
        "text": "Very Poor - This hypothesis fails to meaningfully identify bad data",
        "score": 0.0
      }
    },
    "timestamp": "2025-09-03 21:34:59",
    "is_unique": true
  },
  "overall_verdict": {
    "overall_score": 0.0,
    "best_hypotheses": [],
    "recommendation": "No effective hypotheses found. Consider different sampling approach."
  }
}